{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36c4ec63-dba7-419c-8004-49e080dad6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0fcac89-c063-4e94-bab7-4758348770ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all code below is from https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17d62e9a-808e-4f59-b08d-4763cc12a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdbd9c97-dbf4-4cac-af5a-c1bd539ce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        # d_model: Dimensionality of the input.\n",
    "        # num_heads: The number of attention heads to split the input into.\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a32f5bb5-5991-4b9e-b823-57dce502fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e971111f-40d4-4218-838e-6a490f36afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f02803d-0357-4173-8490-67968b6115b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a4b3208-879d-4df2-9773-59f6dd22d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.patch_projection = nn.Linear(768, d_model)  # Add a linear projection layer\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        batch_size, src_len, _ = src.size()\n",
    "        tgt_len = tgt.size(1)\n",
    "        src_mask = torch.ones(batch_size, 1, 1, src_len).bool().to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, tgt_len, tgt_len), diagonal=1)).bool().to(tgt.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_projected = self.patch_projection(src)  # Project the image patches to the desired dimension\n",
    "        src_embedded = self.dropout(self.positional_encoding(src_projected))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80ecaf55-c8c5-4ad8-8122-59cbc9921925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (2556, 1179, 3)\n",
      "Processed patches shape: torch.Size([256, 768])\n",
      "Processed patches: tensor([[0.4902, 0.4980, 0.5137,  ..., 0.1020, 0.1216, 0.2471],\n",
      "        [0.1020, 0.1216, 0.2471,  ..., 0.1020, 0.1137, 0.2353],\n",
      "        [0.1020, 0.1137, 0.2353,  ..., 0.0980, 0.1137, 0.2275],\n",
      "        ...,\n",
      "        [0.0510, 0.0588, 0.1098,  ..., 0.0471, 0.0588, 0.1020],\n",
      "        [0.0471, 0.0549, 0.1020,  ..., 0.0392, 0.0549, 0.0902],\n",
      "        [0.0392, 0.0471, 0.0941,  ..., 0.4902, 0.4980, 0.5176]])\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, target_size=(256, 256), patch_size=16):\n",
    "    # A patch size of 16 means that each patch is 16 pixels tall and 16 pixels wide\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    resized_image = image.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    image_array = np.array(resized_image)\n",
    "    # print(\"image_array.shape\", image_array.shape)\n",
    "    \n",
    "    # Split the image into patches\n",
    "    patches = image_array.reshape(target_size[0] // patch_size, patch_size,\n",
    "                                  target_size[1] // patch_size, patch_size,\n",
    "                                  image_array.shape[2]).swapaxes(1, 2).reshape(-1, patch_size, patch_size, image_array.shape[2])\n",
    "\n",
    "    # print(\"patches.shape\", patches.shape)\n",
    "    \n",
    "    # Flatten the patches\n",
    "    flattened_patches = patches.reshape(patches.shape[0], -1)\n",
    "    \n",
    "    # Normalize the flattened patches\n",
    "    normalized_patches = flattened_patches / 255.0\n",
    "\n",
    "    # Return a tensor instead of a numpy array:\n",
    "    return torch.tensor(normalized_patches, dtype=torch.float32)\n",
    "\n",
    "# Example usage\n",
    "image_path = \"./flashcard.jpg\"\n",
    "processed_patches = process_image(image_path)\n",
    "\n",
    "print(f\"Original image shape: {np.array(Image.open(image_path)).shape}\")\n",
    "print(f\"Processed patches shape: {processed_patches.shape}\")\n",
    "print(f\"Processed patches: {processed_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60c0b943-64cd-4e39-a1d1-eef1e3a441cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(data.Dataset):\n",
    "    def __init__(self, image_paths, captions, tokenizer, patch_size=16, target_size=(256, 256)):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.patch_size = patch_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        caption = self.captions[index]\n",
    "        image_patches = process_image(image_path, self.target_size, self.patch_size)\n",
    "        # Tokenize the caption\n",
    "        caption_tokens = self.tokenizer.tokenize(caption)\n",
    "        # Add start and end tokens\n",
    "        caption_input = ['[CLS]'] + caption_tokens\n",
    "        caption_label = caption_tokens + ['[SEP]']\n",
    "        # Convert tokens to ids\n",
    "        caption_input_ids = self.tokenizer.convert_tokens_to_ids(caption_input)\n",
    "        caption_label_ids = self.tokenizer.convert_tokens_to_ids(caption_label)\n",
    "        return image_patches, torch.tensor(caption_input_ids), torch.tensor(caption_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fbe531de-9013-414b-82d7-1c55f0128cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example usage\n",
    "# image_paths = [\"./flashcard.jpg\", \"./not-a-flashcard.jpg\"]\n",
    "image_paths = [\"./flashcard.jpg\", \"./flashcard.jpg\", \"./flashcard.jpg\", \"./not-a-flashcard.jpg\", \"./not-a-flashcard.jpg\", \"./not-a-flashcard.jpg\"]\n",
    "captions = [\"this is a flashcard\", \"this is a flashcard\", \"this is a flashcard\", \"this is not a flashcard\", \"this is not a flashcard\", \"this is not a flashcard\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_patches = [item[0] for item in batch]\n",
    "    caption_inputs = [item[1] for item in batch]\n",
    "    caption_labels = [item[2] for item in batch]\n",
    "    \n",
    "    # Pad the caption inputs and labels\n",
    "    padded_caption_inputs = pad_sequence(caption_inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_caption_labels = pad_sequence(caption_labels, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n",
    "    \n",
    "    return torch.stack(image_patches), padded_caption_inputs, padded_caption_labels\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = ImageCaptionDataset(image_paths, captions, tokenizer)\n",
    "# data_loader = data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# use this one when I need to pad:\n",
    "data_loader = data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02fb30fd-9aa9-40ac-90c4-ad7c86f94440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the stuff that's in the Dataset:\n",
    "# image_patches_1 = process_image(\"./flashcard.jpg\")\n",
    "# image_patches_2 = process_image(\"./not-a-flashcard.jpg\")\n",
    "# print(\"image_patches_2\",image_patches_2)\n",
    "# ^ that's all good!\n",
    "\n",
    "# caption = \"this is a flashcard\"\n",
    "# caption_tokens = tokenizer.tokenize(caption)\n",
    "# print(\"caption_tokens\", caption_tokens)\n",
    "# caption_ids = tokenizer.encode(caption, add_special_tokens=True)\n",
    "# print(\"caption_ids\", caption_ids)\n",
    "# ^ that's all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f94c87c-8532-4ef8-8e63-15e4e2ee8107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_vocab_size 30522\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 256  # Set max_seq_length to the number of image patches\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"tgt_vocab_size\", tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d3536add-eeab-4dba-b8d2-20696f9de5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformer model\n",
    "model = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0f171dc-65f9-4151-bd90-01a08a68cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 6.0499\n",
      "Epoch [2/10], Loss: 4.1341\n",
      "Epoch [3/10], Loss: 2.6640\n",
      "Epoch [4/10], Loss: 1.1323\n",
      "Epoch [5/10], Loss: 0.6918\n",
      "Epoch [6/10], Loss: 0.6636\n",
      "Epoch [7/10], Loss: 0.3595\n",
      "Epoch [8/10], Loss: 0.3663\n",
      "Epoch [9/10], Loss: 0.2378\n",
      "Epoch [10/10], Loss: 0.1614\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        image_patches, caption_inputs, caption_labels = batch\n",
    "        image_patches = image_patches.float()\n",
    "\n",
    "        # print(\"image_patches\", image_patches.shape)\n",
    "        # print(\"caption_inputs\", caption_inputs.shape)\n",
    "        # print(\"caption_labels\", caption_labels.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(image_patches, caption_inputs)\n",
    "        \n",
    "        # Compute loss and perform backward pass\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), caption_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c2995981-1872-4f15-9c4c-a7ecb67748cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5364c060-8f4c-4bfb-b31d-2bd82835e945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (patch_projection): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (decoder_embedding): Embedding(30522, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=30522, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new instance of the model\n",
    "trained_model = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Load the trained model's state dictionary\n",
    "trained_model.load_state_dict(torch.load('trained_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abf0f47c-7c07-48bb-ab3d-62c007912d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_image_patches tensor([[[0.5294, 0.5137, 0.4706,  ..., 0.5333, 0.5176, 0.4745],\n",
      "         [0.5333, 0.5176, 0.4745,  ..., 0.5373, 0.5216, 0.4784],\n",
      "         [0.5373, 0.5216, 0.4863,  ..., 0.5412, 0.5255, 0.4824],\n",
      "         ...,\n",
      "         [0.5451, 0.5412, 0.5255,  ..., 0.5294, 0.5255, 0.5059],\n",
      "         [0.5255, 0.5216, 0.5020,  ..., 0.5216, 0.5098, 0.4784],\n",
      "         [0.5176, 0.5020, 0.4627,  ..., 0.5294, 0.5137, 0.4706]]])\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the input image\n",
    "input_image_path = './not-a-flashcard.jpg'\n",
    "input_image_patches = process_image(input_image_path)\n",
    "\n",
    "# Add batch dimension to the image patches\n",
    "input_image_patches = input_image_patches.unsqueeze(0)\n",
    "\n",
    "print(\"input_image_patches\", input_image_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "466465e1-a224-460f-9ab6-aeda53fc48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: this is not a flashcard\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum length for the generated caption\n",
    "max_caption_length = 20\n",
    "\n",
    "# Initialize the caption with the start token\n",
    "generated_caption = [tokenizer.cls_token_id]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_caption_length):\n",
    "        # Convert the generated caption to a tensor\n",
    "        caption_tensor = torch.tensor(generated_caption).unsqueeze(0)\n",
    "\n",
    "        # Generate the next token in the caption\n",
    "        output = trained_model(input_image_patches, caption_tensor)\n",
    "        predicted_token = output[0, -1].argmax(dim=0)\n",
    "\n",
    "        # Append the predicted token to the generated caption\n",
    "        generated_caption.append(predicted_token.item())\n",
    "\n",
    "        # Stop generation if the end token is predicted\n",
    "        if predicted_token.item() == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "# Convert the generated caption tokens to text\n",
    "generated_text = tokenizer.decode(generated_caption[1:-1])\n",
    "\n",
    "print(\"Generated Caption:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5cdb1-5f08-4ce4-89a7-6e03aff6c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
