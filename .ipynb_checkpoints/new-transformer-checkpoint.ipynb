{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489fd2b-b803-420f-b5f7-301e203090f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26358b20-b446-455a-9a26-a635192f22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "from datasets import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02244d5c-3a58-41c4-bbcb-2ce5224c2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2230a-c174-40ef-868f-b9f2704bba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e72ae-a67b-4ce8-9e27-40c170d89056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a DatasetDict with separate splits based on the 'split' column\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset['test'].filter(lambda x: x['split'] == 'train'),\n",
    "    'validation': dataset['test'].filter(lambda x: x['split'] == 'val'),\n",
    "    'test': dataset['test'].filter(lambda x: x['split'] == 'test')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade675f9-5306-4566-b1d7-b32b7d960468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all code below is from https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa751d-c2c3-46c5-a855-166ea9d694fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78433c0-92c8-4d11-bcbd-f429a421785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951d255-2ef0-4b44-9721-4be45543a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        # d_model: Dimensionality of the input.\n",
    "        # num_heads: The number of attention heads to split the input into.\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188a36b-52bc-4150-8010-631473de6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe66358-a5ff-4db5-b7cd-a23d9cbe4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180adff1-5beb-4dc0-a373-fe3d3fff736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd55ea3-5124-4aef-80e3-1384a1bcd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.patch_projection = nn.Linear(768, d_model)  # Add a linear projection layer\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        batch_size, src_len, _ = src.size()\n",
    "        tgt_len = tgt.size(1)\n",
    "        src_mask = torch.ones(batch_size, 1, 1, src_len).bool().to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, tgt_len, tgt_len), diagonal=1)).bool().to(tgt.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_projected = self.patch_projection(src)  # Project the image patches to the desired dimension\n",
    "        src_embedded = self.dropout(self.positional_encoding(src_projected))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93b77f-517d-4366-81a3-11ca1cbd5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS ANYMORE\n",
    "\n",
    "def old_process_image(image_path, target_size=(256, 256), patch_size=16):\n",
    "    # A patch size of 16 means that each patch is 16 pixels tall and 16 pixels wide\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    resized_image = image.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    image_array = np.array(resized_image)\n",
    "    # print(\"image_array.shape\", image_array.shape)\n",
    "    \n",
    "    # Split the image into patches\n",
    "    patches = image_array.reshape(target_size[0] // patch_size, patch_size,\n",
    "                                  target_size[1] // patch_size, patch_size,\n",
    "                                  image_array.shape[2]).swapaxes(1, 2).reshape(-1, patch_size, patch_size, image_array.shape[2])\n",
    "\n",
    "    # print(\"patches.shape\", patches.shape)\n",
    "    \n",
    "    # Flatten the patches\n",
    "    flattened_patches = patches.reshape(patches.shape[0], -1)\n",
    "    \n",
    "    # Normalize the flattened patches\n",
    "    normalized_patches = flattened_patches / 255.0\n",
    "\n",
    "    # Return a tensor instead of a numpy array:\n",
    "    return torch.tensor(normalized_patches, dtype=torch.float32).to(device)\n",
    "\n",
    "# # Example usage\n",
    "# image_path = \"./flashcard.jpg\"\n",
    "# processed_patches = old_process_image(image_path)\n",
    "\n",
    "# print(f\"Original image shape: {np.array(Image.open(image_path)).shape}\")\n",
    "# print(f\"Processed patches shape: {processed_patches.shape}\")\n",
    "# print(f\"Processed patches: {processed_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303e078-7694-42a2-bc20-f4032921d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, target_size=(256, 256), patch_size=16):\n",
    "    # A patch size of 16 means that each patch is 16 pixels tall and 16 pixels wide\n",
    "    # Resize the image to the target size\n",
    "    resized_image = image.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    image_array = np.array(resized_image)\n",
    "    \n",
    "    # Split the image into patches\n",
    "    patches = image_array.reshape(target_size[0] // patch_size, patch_size,\n",
    "                                  target_size[1] // patch_size, patch_size,\n",
    "                                  image_array.shape[2]).swapaxes(1, 2).reshape(-1, patch_size, patch_size, image_array.shape[2])\n",
    "    \n",
    "    # Flatten the patches\n",
    "    flattened_patches = patches.reshape(patches.shape[0], -1)\n",
    "    \n",
    "    # Normalize the flattened patches\n",
    "    normalized_patches = flattened_patches / 255.0\n",
    "    \n",
    "    # Return a tensor instead of a numpy array:\n",
    "    return torch.tensor(normalized_patches, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c9a9f-495a-482e-a95a-26a5492534b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0a687-6c29-4bbf-86f1-69de4aa8d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    image = example['image']\n",
    "    caption = example['caption'][0]  # Use only the first caption\n",
    "    \n",
    "    # Process the image\n",
    "    image_patches = process_image(image)\n",
    "    \n",
    "    # Tokenize the caption\n",
    "    caption_tokens = tokenizer.tokenize(caption)\n",
    "    \n",
    "    # Add start and end tokens\n",
    "    caption_input = [tokenizer.cls_token] + caption_tokens\n",
    "    caption_label = caption_tokens + [tokenizer.sep_token]\n",
    "    \n",
    "    # Convert tokens to ids\n",
    "    caption_input_ids = tokenizer.convert_tokens_to_ids(caption_input)\n",
    "    caption_label_ids = tokenizer.convert_tokens_to_ids(caption_label)\n",
    "    \n",
    "    return {\n",
    "        'image_patches': image_patches,\n",
    "        'caption_input_ids': caption_input_ids,\n",
    "        'caption_label_ids': caption_label_ids\n",
    "    }\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_dataset = dataset_dict.map(preprocess_dataset, batched=False, remove_columns=dataset_dict['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e348653-d7f0-4ddf-aac3-31c8c8569729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(data.Dataset):\n",
    "    def __init__(self, image_paths, captions, tokenizer, patch_size=16, target_size=(256, 256)):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.patch_size = patch_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        caption = self.captions[index]\n",
    "        image_patches = process_image(image_path, self.target_size, self.patch_size)\n",
    "        # Tokenize the caption\n",
    "        caption_tokens = self.tokenizer.tokenize(caption)\n",
    "        # Add start and end tokens\n",
    "        caption_input = ['[CLS]'] + caption_tokens\n",
    "        caption_label = caption_tokens + ['[SEP]']\n",
    "        # Convert tokens to ids\n",
    "        caption_input_ids = self.tokenizer.convert_tokens_to_ids(caption_input)\n",
    "        caption_label_ids = self.tokenizer.convert_tokens_to_ids(caption_label)\n",
    "        return image_patches, torch.tensor(caption_input_ids), torch.tensor(caption_label_ids).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74127d75-5cba-46f6-8d77-cecd249fea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# image_paths = [\"./flashcard.jpg\", \"./flashcard.jpg\", \"./flashcard.jpg\", \"./not-a-flashcard.jpg\", \"./not-a-flashcard.jpg\", \"./not-a-flashcard.jpg\"]\n",
    "# captions = [\"this is a flashcard\", \"this is a flashcard\", \"this is a flashcard\", \"this is not a flashcard\", \"this is not a flashcard\", \"this is not a flashcard\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_patches = [item['image_patches'] for item in batch]\n",
    "    caption_inputs = [item['caption_input_ids'] for item in batch]\n",
    "    caption_labels = [item['caption_label_ids'] for item in batch]\n",
    "    \n",
    "    # Convert image patches, caption inputs, and labels to tensors\n",
    "    image_patches = [torch.tensor(patch) for patch in image_patches]\n",
    "    caption_inputs = [torch.tensor(caption) for caption in caption_inputs]\n",
    "    caption_labels = [torch.tensor(caption) for caption in caption_labels]\n",
    "    \n",
    "    # Pad the caption inputs and labels to the same length\n",
    "    caption_inputs = pad_sequence(caption_inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    caption_labels = pad_sequence(caption_labels, batch_first=True, padding_value=-100)  # -100 is the ignore index for CrossEntropyLoss\n",
    "    \n",
    "    # Stack the image patches\n",
    "    image_patches = torch.stack(image_patches)\n",
    "    \n",
    "    return image_patches.to(device), caption_inputs.to(device), caption_labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0920924-1ad6-4cf8-9d77-b714a58f4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(processed_dataset['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(processed_dataset['validation'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(processed_dataset['test'], batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb008087-4bd5-4782-b0f3-36d56736c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS ANYMORE:\n",
    "\n",
    "# # Create the dataset and data loader\n",
    "# old_dataset = ImageCaptionDataset(image_paths, captions, tokenizer)\n",
    "# # data_loader = data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# # use this one when I need to pad:\n",
    "# old_data_loader = data.DataLoader(old_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65244f43-0594-467e-99b5-80f09bdfd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the stuff that's in the Dataset:\n",
    "# image_patches_1 = process_image(\"./flashcard.jpg\")\n",
    "# image_patches_2 = process_image(\"./not-a-flashcard.jpg\")\n",
    "# print(\"image_patches_2\",image_patches_2)\n",
    "# ^ that's all good!\n",
    "\n",
    "# caption = \"this is a flashcard\"\n",
    "# caption_tokens = tokenizer.tokenize(caption)\n",
    "# print(\"caption_tokens\", caption_tokens)\n",
    "# caption_ids = tokenizer.encode(caption, add_special_tokens=True)\n",
    "# print(\"caption_ids\", caption_ids)\n",
    "# ^ that's all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f584e-324d-4304-aeb3-bc871f817d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 256  # Set max_seq_length to the number of image patches\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"tgt_vocab_size\", tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c6633-d350-4d88-9183-a6b42d6aca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformer model\n",
    "model = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779960d-c8c1-4c99-83ea-febe797e9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", unit=\"batch\")\n",
    "    \n",
    "    # Initialize variables for tracking training progress\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        image_patches, caption_inputs, caption_labels = batch\n",
    "        image_patches = image_patches.float()\n",
    "        \n",
    "        # Move tensors to the appropriate device (CPU or GPU)\n",
    "        image_patches = image_patches.to(device)\n",
    "        caption_inputs = caption_inputs.to(device)\n",
    "        caption_labels = caption_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image_patches, caption_inputs)\n",
    "        \n",
    "        # Compute loss and perform backward pass\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), caption_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update training progress variables\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Calculate and print average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa17166-ef31-4a7b-bae8-38132db54f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723f784-b362-4d10-a3ac-9cf331e3e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the model\n",
    "trained_model = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "# Load the trained model's state dictionary\n",
    "trained_model.load_state_dict(torch.load('trained_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918aec21-073e-4906-a446-969d35d371cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "input_image_path1 = './dog.jpeg'\n",
    "input_image_path2 = './2men.jpeg'\n",
    "input_image_path3 = './woman.jpeg'\n",
    "input_image_path4 = './cat.jpeg'\n",
    "input_image_path5 = './cat2.jpeg'\n",
    "input_image_path6 = './dog2.jpeg'\n",
    "input_image_path7 = './girl.jpeg'\n",
    "input_image_path8 = './blue.jpeg'\n",
    "input_image_path9 = './red.jpeg'\n",
    "input_image_path10 = './dog3.jpeg'\n",
    "input_image_path11 = './dog4.jpeg'\n",
    "input_image_path12 = './dogg.png'\n",
    "input_image_patches = old_process_image(input_image_path12)\n",
    "# Add batch dimension to the image patches\n",
    "input_image_patches = input_image_patches.unsqueeze(0)\n",
    "# Move the input tensor to the same device as the model\n",
    "input_image_patches = input_image_patches.to(device)\n",
    "\n",
    "\n",
    "print(\"input_image_patches\", input_image_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c442f1-5ec0-4413-8479-b3e553514ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum length for the generated caption\n",
    "max_caption_length = 20\n",
    "\n",
    "# Initialize the caption with the start token\n",
    "generated_caption = [tokenizer.cls_token_id]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_caption_length):\n",
    "        # Convert the generated caption to a tensor\n",
    "        caption_tensor = torch.tensor(generated_caption).unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the next token in the caption\n",
    "        output = trained_model(input_image_patches, caption_tensor)\n",
    "        predicted_token = output[0, -1].argmax(dim=0)\n",
    "\n",
    "        # Move the predicted token to the CPU before appending it to the generated caption\n",
    "        predicted_token = predicted_token.cpu()\n",
    "\n",
    "        # Append the predicted token to the generated caption\n",
    "        generated_caption.append(predicted_token.item())\n",
    "\n",
    "        # Stop generation if the end token is predicted\n",
    "        if predicted_token.item() == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "# Convert the generated caption tokens to text\n",
    "generated_text = tokenizer.decode(generated_caption[1:-1])\n",
    "\n",
    "print(\"Generated Caption:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
